{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190f2a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paige\\anaconda3\\envs\\fid\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# add the path\n",
    "sys.path.append(\"run\")\n",
    "from base_utils import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModel, BartForConditionalGeneration, BertForSequenceClassification\n",
    "import argparse\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb8f946-2718-4c74-a38b-dff84efe2aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e104330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        for key in sample.keys():\n",
    "            if key == \"input\": \n",
    "                text = self.tokenizer(sample[key])[\"input_ids\"]\n",
    "            if key == \"target\":\n",
    "                if sample[key]: #True\n",
    "                    target = 1  \n",
    "                else:\n",
    "                    target = 0\n",
    "        return_dict = {\"input_ids\": torch.tensor(text), \n",
    "                        \"target\": torch.tensor([target])}\n",
    "        return return_dict\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "path = \"./train_classifier_data/\"\n",
    "train = os.path.join(path, \"new_train.csv\")\n",
    "valid = os.path.join(path, \"new_valid.csv\")\n",
    "test = os.path.join(path, \"new_test.csv\")\n",
    "data = load_dataset(\"csv\", data_files={\"train\": train,\n",
    "                                       \"valid\": valid,\n",
    "                                       \"test\": test})\n",
    "batch_size = 8\n",
    "processor = Processor(tokenizer)\n",
    "encoded_data = data.map(lambda sample: processor(sample))\n",
    "encoded_data.set_format(\"torch\")\n",
    "# batchify the encoded data\n",
    "train_dataloader = batchify(encoded_data[\"train\"][\"input_ids\"], encoded_data[\"train\"][\"target\"],\n",
    "                            batch_size=batch_size)\n",
    "valid_dataloader = batchify(encoded_data[\"valid\"][\"input_ids\"], encoded_data[\"valid\"][\"target\"],\n",
    "                            batch_size=batch_size)\n",
    "test_dataloader = batchify(encoded_data[\"test\"][\"input_ids\"], encoded_data[\"test\"][\"target\"],\n",
    "                           batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04669dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "The total number of parameters is: 109.48M\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 0 -------------------------\n",
      "on train data\n",
      "loss: 0.5994563102722168\n",
      "precision: 0.6886792778968811\n",
      "------------------------- epoch: 0 -------------------------\n",
      "on valid data\n",
      "loss: 0.625410258769989\n",
      "precision: 0.6018018126487732\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 1 -------------------------\n",
      "on train data\n",
      "loss: 0.5041234493255615\n",
      "precision: 0.7838050127029419\n",
      "------------------------- epoch: 1 -------------------------\n",
      "on valid data\n",
      "loss: 0.5068817734718323\n",
      "precision: 0.8247104287147522\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 2 -------------------------\n",
      "on train data\n",
      "loss: 0.43047645688056946\n",
      "precision: 0.8176101446151733\n",
      "------------------------- epoch: 2 -------------------------\n",
      "on valid data\n",
      "loss: 0.447398841381073\n",
      "precision: 0.8048906922340393\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 3 -------------------------\n",
      "on train data\n",
      "loss: 0.3688967823982239\n",
      "precision: 0.8466981053352356\n",
      "------------------------- epoch: 3 -------------------------\n",
      "on valid data\n",
      "loss: 0.41383323073387146\n",
      "precision: 0.7971686720848083\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 4 -------------------------\n",
      "on train data\n",
      "loss: 0.31874653697013855\n",
      "precision: 0.865566074848175\n",
      "------------------------- epoch: 4 -------------------------\n",
      "on valid data\n",
      "loss: 0.4002273380756378\n",
      "precision: 0.8320785760879517\n"
     ]
    }
   ],
   "source": [
    "class Classifier(JoModule):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        logits = self.base(input_ids).logits\n",
    "        return logits   # in shape (N, 2)\n",
    "    \n",
    "    def training_step(self, batch, device):\n",
    "        input_ids, target = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        target = target.reshape(-1).to(device)\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.loss_fn(logits, target)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, device, metrics=[\"loss\", \"precision\"]):\n",
    "        input_ids, target = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        target = target.reshape(-1).to(device)\n",
    "        logits = self.forward(input_ids)\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(logits, target)\n",
    "        # compute precision\n",
    "        precision = (logits.topk(1).indices.reshape(-1) == target).sum() / len(target)\n",
    "        return {\"loss\": loss, \"precision\": precision}\n",
    "    \n",
    "base = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model = Classifier(base)\n",
    "trainer = Trainer(batch_size=8,\n",
    "                  max_epochs=5,\n",
    "                  optimizer_method=\"Adam\",\n",
    "                  lr=2e-6,\n",
    "                  save_model=\"exp_classifier_new\",\n",
    "                  logging=\"exp_classifier.log\",\n",
    "                  use_amp=False,\n",
    "                  warmup=False,\n",
    "                  accelerator=\"cuda:0\",\n",
    "                  valid_metrics=[\"loss\", \"precision\"],\n",
    "                  efficient_valid=True, \n",
    "                  )\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a19292d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'loss': tensor(0.4179, device='cuda:0'),\n",
       "             'precision': tensor(0.8044, device='cuda:0')})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(base)\n",
    "model = load_model(model, \"./saved/exp_classifier_new_dict\")\n",
    "model.validIter(test_dataloader, \"cuda:0\", [\"loss\", \"precision\"], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc677b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
