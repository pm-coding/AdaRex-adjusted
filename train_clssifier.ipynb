{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190f2a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu11/s0/pmm2776/miniconda3/envs/adarex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# add the path\n",
    "sys.path.append(\"run\")\n",
    "from base_utils import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModel, BartForConditionalGeneration, BertForSequenceClassification\n",
    "import argparse\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb8f946-2718-4c74-a38b-dff84efe2aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e104330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor():\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        for key in sample.keys():\n",
    "            if key == \"input\": \n",
    "                text = self.tokenizer(sample[key])[\"input_ids\"]\n",
    "            if key == \"target\":\n",
    "                if sample[key]: #True\n",
    "                    target = 1  \n",
    "                else:\n",
    "                    target = 0\n",
    "        return_dict = {\"input_ids\": torch.tensor(text), \n",
    "                        \"target\": torch.tensor([target])}\n",
    "        return return_dict\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "path = \"./train_classifier_data/\"\n",
    "train = os.path.join(path, \"new_train.csv\")\n",
    "valid = os.path.join(path, \"new_valid.csv\")\n",
    "test = os.path.join(path, \"new_test.csv\")\n",
    "data = load_dataset(\"csv\", data_files={\"train\": train,\n",
    "                                       \"valid\": valid,\n",
    "                                       \"test\": test})\n",
    "batch_size = 128\n",
    "processor = Processor(tokenizer)\n",
    "encoded_data = data.map(lambda sample: processor(sample))\n",
    "encoded_data.set_format(\"torch\")\n",
    "# batchify the encoded data\n",
    "train_dataloader = batchify(encoded_data[\"train\"][\"input_ids\"], encoded_data[\"train\"][\"target\"],\n",
    "                            batch_size=batch_size)\n",
    "valid_dataloader = batchify(encoded_data[\"valid\"][\"input_ids\"], encoded_data[\"valid\"][\"target\"],\n",
    "                            batch_size=batch_size)\n",
    "test_dataloader = batchify(encoded_data[\"test\"][\"input_ids\"], encoded_data[\"test\"][\"target\"],\n",
    "                           batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60e55608-00f5-4e1f-a18b-110be289ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04669dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "The total number of parameters is: 109.48M\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 0 -------------------------\n",
      "on train data\n",
      "loss: 0.7017771601676941\n",
      "precision: 0.4909762144088745\n",
      "------------------------- epoch: 0 -------------------------\n",
      "on valid data\n",
      "loss: 0.67826247215271\n",
      "precision: 0.5450066328048706\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 1 -------------------------\n",
      "on train data\n",
      "loss: 0.6633497476577759\n",
      "precision: 0.6096197366714478\n",
      "------------------------- epoch: 1 -------------------------\n",
      "on valid data\n",
      "loss: 0.648210883140564\n",
      "precision: 0.7734954357147217\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 2 -------------------------\n",
      "on train data\n",
      "loss: 0.6125339865684509\n",
      "precision: 0.7670373916625977\n",
      "------------------------- epoch: 2 -------------------------\n",
      "on valid data\n",
      "loss: 0.6069641709327698\n",
      "precision: 0.7939589023590088\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 3 -------------------------\n",
      "on train data\n",
      "loss: 0.5285905599594116\n",
      "precision: 0.7923558950424194\n",
      "------------------------- epoch: 3 -------------------------\n",
      "on valid data\n",
      "loss: 0.5451961755752563\n",
      "precision: 0.8159466981887817\n",
      "current learning rate: 2e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- epoch: 4 -------------------------\n",
      "on train data\n",
      "loss: 0.46761685609817505\n",
      "precision: 0.8470154404640198\n",
      "------------------------- epoch: 4 -------------------------\n",
      "on valid data\n",
      "loss: 0.48416590690612793\n",
      "precision: 0.8179308772087097\n",
      "Time taken:48.646119356155396 seconds\n"
     ]
    }
   ],
   "source": [
    "class Classifier(JoModule):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        logits = self.base(input_ids).logits\n",
    "        return logits   # in shape (N, 2)\n",
    "    \n",
    "    def training_step(self, batch, device):\n",
    "        input_ids, target = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        target = target.reshape(-1).to(device)\n",
    "        logits = self.forward(input_ids)\n",
    "        loss = self.loss_fn(logits, target)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, device, metrics=[\"loss\", \"precision\"]):\n",
    "        input_ids, target = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        target = target.reshape(-1).to(device)\n",
    "        logits = self.forward(input_ids)\n",
    "        # compute loss\n",
    "        loss = self.loss_fn(logits, target)\n",
    "        # compute precision\n",
    "        precision = (logits.topk(1).indices.reshape(-1) == target).sum() / len(target)\n",
    "        return {\"loss\": loss, \"precision\": precision}\n",
    "start = time.time()    \n",
    "base = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "model = Classifier(base)\n",
    "trainer = Trainer(batch_size=128,\n",
    "                  max_epochs=5,\n",
    "                  optimizer_method=\"Adam\",\n",
    "                  lr=2e-6,\n",
    "                  save_model=\"exp_classifier_new\",\n",
    "                  logging=\"exp_classifier.log\",\n",
    "                  use_amp=False,\n",
    "                  warmup=False,\n",
    "                  accelerator=\"cuda:0\",\n",
    "                  valid_metrics=[\"loss\", \"precision\"],\n",
    "                  efficient_valid=True, \n",
    "                  )\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)\n",
    "end = time.time()\n",
    "print(\"Time taken:\"+str(end-start)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a19292d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:0.5799984931945801 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = Classifier(base)\n",
    "model = load_model(model, \"./saved/exp_classifier_new_dict\")\n",
    "model.validIter(test_dataloader, \"cuda:0\", [\"loss\", \"precision\"], False)\n",
    "end = time.time()\n",
    "print(\"Time taken:\" + str(end-start)+\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc677b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
